{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS:GO Player Form Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accompany our article/blog, this notebook is designed to show how we can use the simple variables such as the number of kills and deaths a player has in a given game in order to predict how they \"should\" perform on their next game. We first do a bunch of preprocessing and data manipulation. Then we use a recurrent neural network (specifically LSTM) to predict our measure of player form. Our real model of player form incorporates many more variables that we have extracted from the demo files of professional games, which give greater insight into player form than kills/deaths alone.\n",
    "\n",
    "But anyway here is an open and fun way to look at using data to predict the outcome of professional CSGO matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "# Python imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import theano as th\n",
    "from theano import tensor as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Le data imports* \n",
    "\n",
    "Clearly you need to have the correct \"matchdata\" csv file in the same directory as this notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maximilian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (82,83,87) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv('matchdata.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Names...\n",
    "\n",
    "Here we can see a nice list of all the variables that our data gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(dat.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need a function that pulls out only a certain teams data:\n",
    "\n",
    "def team_data(team): # probably by index...\n",
    "    df = dat.loc[(dat.ix[:,3] == team) |  (dat.ix[:,4] == team) ]\n",
    "    return(df)\n",
    "\n",
    "# make a player data one...\n",
    "\n",
    "def player_data(player): # can define by indx later as well...\n",
    "    df = dat.loc[(dat.ix[:,11] == player) |  (dat.ix[:,17] == player)|  (dat.ix[:,23] == player)|  (dat.ix[:,29] == player)|  (dat.ix[:,35] == player)|  (dat.ix[:,41] == player) |  (dat.ix[:,47] == player) |  (dat.ix[:,53] == player) |  (dat.ix[:,59] == player) |  (dat.ix[:,65] == player)]\n",
    "    return(df)\n",
    "\n",
    "# make function that says which team a player is on (1 or 2)...\n",
    "def player_side(player, match): # match is the match # as integer\n",
    "\n",
    "    dato = player_data(player).reset_index(drop=True) \n",
    "    N = len(dato)\n",
    "    i = match\n",
    "    \n",
    "    side = []\n",
    "    # loop over each row...\n",
    "    if( (dato.ix[i,11] == player) |  (dato.ix[i,17] == player)|  (dato.ix[i,23] == player)|  (dato.ix[i,29] == player)|  (dato.ix[i,35] == player) ):\n",
    "        side = 1\n",
    "    elif( (dato.ix[i,41] == player) |  (dato.ix[i,47] == player) |  (dato.ix[i,53] == player) |  (dato.ix[i,59] == player) |  (dato.ix[i,65] == player) ):\n",
    "        side = 2\n",
    "    else:\n",
    "        side = 0\n",
    "    return(side)\n",
    "\n",
    "\n",
    "def team_score(player, match):\n",
    "    '''\n",
    "    Returns team scores, enemy scores and team deaths (TD)...\n",
    "    '''\n",
    "    \n",
    "    side = player_side(player, match)\n",
    "    dato = player_data(player).reset_index(drop=True) \n",
    "    i = match\n",
    "    if(side == 1):\n",
    "        teamscore = dato.ix[match,6] +dato.ix[match,8]\n",
    "        enemyscore = dato.ix[match,7] + dato.ix[match,9]\n",
    "        TK = (dato.ix[i,12]) + (dato.ix[i,18])+  (dato.ix[i,24])+(dato.ix[i,30])+ (dato.ix[i,36])\n",
    "        TD = (dato.ix[i,13] == player) + (dato.ix[i,19])+  (dato.ix[i,25])+(dato.ix[i,31])+ (dato.ix[i,37])\n",
    "        \n",
    "    elif(side ==2 ):\n",
    "        teamscore = dato.ix[match,7] +dato.ix[match,9]\n",
    "        enemyscore = dato.ix[match,6] +dato.ix[match,8]\n",
    "        TK = dato.ix[i,42] + (dato.ix[i,48])+  (dato.ix[i,54])+(dato.ix[i,60] )+ (dato.ix[i,66])\n",
    "        TD = dato.ix[i,43]+ (dato.ix[i,19])+  (dato.ix[i,55])+(dato.ix[i,61] )+ (dato.ix[i,67])\n",
    "        \n",
    "    else:\n",
    "        teamscore = \"ERROR\"\n",
    "        enemyscore = \"UNLUGGY UCE\"\n",
    "        \n",
    "        \n",
    "    return( teamscore, enemyscore, TK, TD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team score is: 16\n",
      "enemy score is: 7\n",
      "team kills are: 89.0\n",
      "team deaths are: 76.0\n"
     ]
    }
   ],
   "source": [
    "TS, ES, tk, td = team_score('GeT_RiGhT', 10)\n",
    "\n",
    "print('team score is:', TS )\n",
    "print('enemy score is:', ES )\n",
    "print('team kills are:', tk )\n",
    "print('team deaths are:', td)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "get_metrics returns the players kills, KDR, teamscore, enemyscore, team kills and enemy kills at specified time...\n",
    "'''\n",
    "def get_metrics(player, match):\n",
    "    TS, ES, tk, td = team_score(player, match)\n",
    "    # get the new player \n",
    "    dato = player_data(player).reset_index(drop=True) \n",
    "    i = match\n",
    "    if( dato.ix[i,11] == player ):\n",
    "        kills = dato.ix[i,12]\n",
    "        deaths = dato.ix[i,13]\n",
    "        kd = dato.ix[i,12]/dato.ix[i,13]    \n",
    "    elif( dato.ix[i,17] == player ):\n",
    "        kills = dato.ix[i,18]\n",
    "        deaths = dato.ix[i,19]\n",
    "        kd = dato.ix[i,18]/dato.ix[i,19]\n",
    "    elif( dato.ix[i,23] == player ):\n",
    "        kills = dato.ix[i,24]    \n",
    "        deaths = dato.ix[i,25]\n",
    "        kd = dato.ix[i,24]/dato.ix[i,25]\n",
    "    elif( dato.ix[i,29] == player ):\n",
    "        kills = dato.ix[i,30]  \n",
    "        deaths = dato.ix[i,31]\n",
    "        kd = dato.ix[i,30]/dato.ix[i,31]\n",
    "    elif( dato.ix[i,35] == player ):\n",
    "        kills = dato.ix[i,36]\n",
    "        deaths = dato.ix[i,37]\n",
    "        kd = dato.ix[i,36]/dato.ix[i,37]\n",
    "    elif( dato.ix[i,41] == player ):\n",
    "        kills = dato.ix[i,42]    \n",
    "        deaths = dato.ix[i,43]\n",
    "        kd = dato.ix[i,42]/dato.ix[i,43]\n",
    "    elif( dato.ix[i,47] == player ):\n",
    "        kills = dato.ix[i,48]       \n",
    "        deaths = dato.ix[i,49]\n",
    "        kd = dato.ix[i,48]/dato.ix[i,49]\n",
    "    elif( dato.ix[i,53] == player ):\n",
    "        kills = dato.ix[i,54]      \n",
    "        deaths = dato.ix[i,55]\n",
    "        kd = dato.ix[i,54]/dato.ix[i,55]\n",
    "    elif( dato.ix[i,59] == player ):\n",
    "        kills = dato.ix[i,60] \n",
    "        deaths = dato.ix[i,61]\n",
    "        kd = dato.ix[i,60]/dato.ix[i,61]\n",
    "    elif( dato.ix[i,65] == player ):\n",
    "        kills = dato.ix[i,66] \n",
    "        deaths = dato.ix[i,67]\n",
    "        kd = dato.ix[i,66]/dato.ix[i,67]        \n",
    "     \n",
    "    k_TK = kills / tk\n",
    "    kDR_A = kd / (np.log(1+TS)*(TS - ES+1))\n",
    "    p_p = np.log(((kills*kills)/(deaths)+1))+np.log(((k_TK)*(kDR_A))/(deaths/td)+1)\n",
    "    \n",
    "    return(kills, deaths, kd, TS, ES, tk ,td, k_TK, kDR_A, p_p)            \n",
    "                \n",
    "'''\n",
    "Now make function that makes CSV/df for the player and their past performance...\n",
    "'''   \n",
    "\n",
    "def make_inputs(player): # uses the player and gets all inputs from this...\n",
    "    datta = player_data(player).reset_index(drop=True) \n",
    "    N = np.shape(datta)[0] # length of the data frame...\n",
    "    \n",
    "    # initialize the vectors...\n",
    "    kill = []\n",
    "    kdr = []\n",
    "    scorediff = []\n",
    "    kill_cont = [] # contribution of kills\n",
    "    death_cont = [] # contriubution of deaths\n",
    "    kdr_adjust = []\n",
    "    player_perf = []\n",
    "    \n",
    "    print(N)\n",
    "    \n",
    "    # loop over each row to get data:\n",
    "    for row in range(N):\n",
    "        k,d,KDR, TS,ES,tk,td,k_TK,kDR_A, p_p = get_metrics('GeT_RiGhT', row)\n",
    "        kill.append(k)\n",
    "        kdr.append(KDR)\n",
    "        scorediff.append(TS-ES)\n",
    "        kill_cont.append(k_TK)\n",
    "        death_cont.append(d/td)\n",
    "        kdr_adjust.append(kDR_A)\n",
    "        player_perf.append(p_p)\n",
    "        \n",
    "    # add all as columns to the dataframe...    \n",
    "    newdat = pd.DataFrame([kill,kdr,scorediff,kill_cont, death_cont, kdr_adjust, player_perf])\n",
    "    \n",
    "    # but data is a tensor - so turn into tensor\n",
    "    \n",
    "    return(newdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.352941</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>1.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.238636</td>\n",
       "      <td>0.159574</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.182692</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.186992</td>\n",
       "      <td>0.280899</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.202381</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.158879</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.215909</td>\n",
       "      <td>0.184211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.151617</td>\n",
       "      <td>0.058826</td>\n",
       "      <td>-0.080425</td>\n",
       "      <td>0.069856</td>\n",
       "      <td>0.079588</td>\n",
       "      <td>0.345646</td>\n",
       "      <td>0.105046</td>\n",
       "      <td>0.117652</td>\n",
       "      <td>-0.085516</td>\n",
       "      <td>-0.049504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029374</td>\n",
       "      <td>-0.016394</td>\n",
       "      <td>-0.075463</td>\n",
       "      <td>-0.088619</td>\n",
       "      <td>-0.042364</td>\n",
       "      <td>0.078951</td>\n",
       "      <td>0.205891</td>\n",
       "      <td>0.045251</td>\n",
       "      <td>0.021435</td>\n",
       "      <td>-0.103403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.031733</td>\n",
       "      <td>2.640053</td>\n",
       "      <td>3.030466</td>\n",
       "      <td>3.226003</td>\n",
       "      <td>3.559819</td>\n",
       "      <td>3.360810</td>\n",
       "      <td>4.673123</td>\n",
       "      <td>4.879253</td>\n",
       "      <td>2.664939</td>\n",
       "      <td>2.474684</td>\n",
       "      <td>...</td>\n",
       "      <td>1.814762</td>\n",
       "      <td>0.962671</td>\n",
       "      <td>2.826264</td>\n",
       "      <td>2.794341</td>\n",
       "      <td>2.474499</td>\n",
       "      <td>2.840884</td>\n",
       "      <td>3.671677</td>\n",
       "      <td>3.304050</td>\n",
       "      <td>2.573486</td>\n",
       "      <td>3.261047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 277 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0  21.000000  15.000000  20.000000  19.000000  23.000000  23.000000   \n",
       "1   1.166667   0.833333   1.111111   1.187500   1.352941   0.958333   \n",
       "2  -4.000000   4.000000  -7.000000   5.000000   5.000000   0.000000   \n",
       "3   0.238636   0.159574   0.303030   0.182692   0.232323   0.186992   \n",
       "4   0.193548   0.246575   0.225000   0.186047   0.195402   0.258065   \n",
       "5  -0.151617   0.058826  -0.080425   0.069856   0.079588   0.345646   \n",
       "6   3.031733   2.640053   3.030466   3.226003   3.559819   3.360810   \n",
       "\n",
       "         6          7          8          9      ...            267       268  \\\n",
       "0  25.000000  30.000000  17.000000  14.000000    ...      10.000000  6.000000   \n",
       "1   3.571429   3.333333   0.850000   0.823529    ...       0.526316  0.272727   \n",
       "2  11.000000   9.000000  -5.000000  -9.000000    ...     -11.000000 -9.000000   \n",
       "3   0.280899   0.312500   0.202381   0.202899    ...       0.172414  0.093750   \n",
       "4   0.159091   0.121622   0.246914   0.193182    ...       0.256757  0.229167   \n",
       "5   0.105046   0.117652  -0.085516  -0.049504    ...      -0.029374 -0.016394   \n",
       "6   4.673123   4.879253   2.664939   2.474684    ...       1.814762  0.962671   \n",
       "\n",
       "         269        270        271        272        273        274  \\\n",
       "0  19.000000  17.000000  15.000000  17.000000  14.000000  15.000000   \n",
       "1   0.904762   1.062500   0.750000   0.894737   2.333333   1.666667   \n",
       "2  -6.000000  -6.000000 -12.000000   3.000000   3.000000  12.000000   \n",
       "3   0.253333   0.346939   0.192308   0.158879   0.168675   0.176471   \n",
       "4   0.265823   0.216216   0.266667   0.220930   0.206897   0.169811   \n",
       "5  -0.075463  -0.088619  -0.042364   0.078951   0.205891   0.045251   \n",
       "6   2.826264   2.794341   2.474499   2.840884   3.671677   3.304050   \n",
       "\n",
       "         275        276  \n",
       "0  15.000000  20.000000  \n",
       "1   0.789474   1.428571  \n",
       "2  12.000000  -7.000000  \n",
       "3   0.211268   0.210526  \n",
       "4   0.215909   0.184211  \n",
       "5   0.021435  -0.103403  \n",
       "6   2.573486   3.261047  \n",
       "\n",
       "[7 rows x 277 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame = make_inputs('fnx') # save as \"frame\" to use in function later - although will really call this from inside the function for \n",
    "# get_tensor\n",
    "#get_metrics('GeT_RiGhT',3)\n",
    "make_inputs('fnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORRRRRRRRRRZZZZZZZZZZZZZ\n",
    "\n",
    "*Le Tensor* construction. Imagine a matrix, now imagine a stack of matrices for each input sample - we also make sure to zero-pad the input matrices, this way we have conformable arrays when implementing the recurrent net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tensor(dataframe, K = 7, T = \"default\"):  # standardize the T window size, and K variables...\n",
    "    '''\n",
    "    T is window size\n",
    "    K is number of input variables...\n",
    "    N is number of input samples...\n",
    "    '''\n",
    "    N = np.shape(dataframe)[1]\n",
    "    if(T == \"default\"):\n",
    "        T = N # standardize T window\n",
    "    # otherwise we have already defined T by function...\n",
    "        \n",
    "    tens = np.zeros((N,T,K)) # iniitalizes tensor shape... (From a given 'N', and known 'T' and 'K')\n",
    "\n",
    "    '''\n",
    "    basically need to generate a new dataframe for each 'N'...\n",
    "    N is given by the number of time steps as well - as T\n",
    "    ...\n",
    "    each dataframe is a TxK array of zeros (using np.zeros)   \n",
    "    '''\n",
    "\n",
    "    # loop over each N:\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        # generate dataframe of zeros\n",
    "        df = np.zeros((T,K)) # this is of size T by K...\n",
    "    \n",
    "        # add for the first one...\n",
    "        df[0] = dataframe.ix[:K-1,N-1] # since frame is a pandas dataframe... (from make_inputs command...)\n",
    "    \n",
    "        for j in range(1,N-i):\n",
    "            df[j] = dataframe.ix[:K-1,N-i-j] # since frame is a pandas dataframe... (from make_inputs command...)\n",
    "        \n",
    "        tens[i] = df # adds to the tensor...\n",
    "    '''\n",
    "    The result of this will be fed into get_targs... (Just FYI)\n",
    "    '''\n",
    "    #return(np.shape(tens)) # return the shape of the tensor - change to the actual tensor if working correct..\n",
    "    return(tens)  # add this instead of above line if want actual tensor output...\n",
    "    # it works!! (well the shape says so) so we will return the actual object instead ^^\n",
    "               \n",
    "        \n",
    "def form_calc(kill,kdr,scorediff,kill_cont, death_cont, kdr_adjust):\n",
    "    '''\n",
    "    This calculates the form of a player given the input variables...\n",
    "    '''\n",
    "    # score = np.log((K**2)/D) + np.log( ( (K/TK)*(K/D)/ np.log(1+TS)*(TS-ES) )/(D/TD) ) # - old version...\n",
    "    # Need to extrapolate team score: - since the winner *usually* gets 16 this gives us\n",
    "    if(scorediff < 0):\n",
    "        TS = 16 + scorediff\n",
    "    else:\n",
    "        TS = scorediff\n",
    "        \n",
    "    score = np.log(kill*kdr) + np.log( ((kill_cont*kdr)/(np.log(1+TS)*scorediff))/death_cont)  # new version\n",
    "    # except we are currently \n",
    "    \n",
    "    return(score)\n",
    "    \n",
    "    # Don't need this anymore...\n",
    "    \n",
    "    \n",
    "def get_targs(tensor): \n",
    "    '''\n",
    "    This function takes the tensor, removes the final entry,\n",
    "    uses that to calculate the \"target\" then returns a tensor of\n",
    "    one less \"N\" size than orignally. It also returns a vector of\n",
    "    targets.\n",
    "    '''\n",
    "    # get N, T and K dimensions:\n",
    "    N = np.shape(tensor)[0]\n",
    "    T = np.shape(tensor)[1]\n",
    "    K = np.shape(tensor)[2]-1  # target is given as the final dimension - so remove...\n",
    "    \n",
    "    # remove last record... ()\n",
    "    newtens = np.delete(tensor,N - 1,0)\n",
    "    \n",
    "    # for each record, split tensor into input and target components... - so \"T\" will drop one dimension...\n",
    "    #targz = np.zeros((N-1,1)) ## this is a vector\n",
    "    targz = []\n",
    "    inputs = np.zeros((N-1,T-1,K)) # this is still a 3D tensor, but dimensions have changed slightly\n",
    "    \n",
    "    for n in range(N-1): # looping over each sample...\n",
    "        '''\n",
    "        inputs are: kill,kdr,scorediff,kill_cont, death_cont, kdr_adjust , output is performance\n",
    "        '''\n",
    "        \n",
    "        targz.append(tensor[n,1,6]/np.mean(tensor[n,1:T-n,6]))\n",
    "        \n",
    "        inputs[n-1] = tensor[n-1,1:T,:K]\n",
    "        \n",
    "        # can comment out the below if required\n",
    "        print('inputs at',n,'are:',tensor[n,1,0],tensor[n,1,1],tensor[n,1,2],tensor[n,1,3],tensor[n,1,4],tensor[n,1,5])\n",
    "        # that should do the trick\n",
    "        \n",
    "    return(targz[:len(targz)-1], inputs[:len(targz)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n"
     ]
    }
   ],
   "source": [
    "TENZ = get_tensor(make_inputs('fnx')) # using the data for \"fnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 277, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(TENZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs at 0 are: 20.0 1.42857142857 -7.0 0.210526315789 0.184210526316 -0.103403448072\n",
      "inputs at 1 are: 15.0 0.789473684211 12.0 0.211267605634 0.215909090909 0.0214345824209\n",
      "inputs at 2 are: 15.0 1.66666666667 12.0 0.176470588235 0.169811320755 0.0452507851109\n",
      "inputs at 3 are: 14.0 2.33333333333 3.0 0.168674698795 0.206896551724 0.205891072254\n",
      "inputs at 4 are: 17.0 0.894736842105 3.0 0.158878504673 0.220930232558 0.0789507119171\n",
      "inputs at 5 are: 15.0 0.75 -12.0 0.192307692308 0.266666666667 -0.0423637455382\n",
      "inputs at 6 are: 17.0 1.0625 -6.0 0.34693877551 0.216216216216 -0.0886193831777\n",
      "inputs at 7 are: 19.0 0.904761904762 -6.0 0.253333333333 0.26582278481 -0.0754630041625\n",
      "inputs at 8 are: 6.0 0.272727272727 -9.0 0.09375 0.229166666667 -0.0163942618283\n",
      "inputs at 9 are: 10.0 0.526315789474 -11.0 0.172413793103 0.256756756757 -0.0293742435027\n",
      "inputs at 10 are: 23.0 1.64285714286 9.0 0.247311827957 0.215384615385 0.0579856489206\n",
      "inputs at 11 are: 15.0 1.07142857143 8.0 0.145631067961 0.269230769231 0.0420185861744\n",
      "inputs at 12 are: 15.0 2.14285714286 14.0 0.185185185185 0.148936170213 0.0504223034093\n",
      "inputs at 13 are: 11.0 1.22222222222 12.0 0.129411764706 0.225 0.0331839090813\n",
      "inputs at 14 are: 14.0 2.0 14.0 0.162790697674 0.241379310345 0.0470608165153\n",
      "inputs at 15 are: 28.0 2.54545454545 8.0 0.277227722772 0.207547169811 0.0998259744264\n",
      "inputs at 16 are: 21.0 1.23529411765 -6.0 0.230769230769 0.2125 -0.103031531999\n",
      "inputs at 17 are: 19.0 0.95 15.0 0.246753246753 0.246913580247 0.0209567698545\n",
      "inputs at 18 are: 19.0 2.71428571429 15.0 0.240506329114 0.225806451613 0.0598764852985\n",
      "inputs at 19 are: 14.0 0.608695652174 -3.0 0.155555555556 0.270588235294 -0.115324446601\n",
      "inputs at 20 are: 20.0 1.33333333333 10.0 0.204081632653 0.234375 0.0427825604685\n",
      "inputs at 21 are: 26.0 2.0 10.0 0.276595744681 0.245283018868 0.0641738407027\n",
      "inputs at 22 are: 22.0 1.0 -2.0 0.207547169811 0.207547169811 -0.369269373069\n",
      "inputs at 23 are: 20.0 1.11111111111 -2.0 0.212765957447 0.163636363636 -0.41029930341\n",
      "inputs at 24 are: 27.0 1.5 5.0 0.278350515464 0.25 0.0882390309662\n",
      "inputs at 25 are: 21.0 3.5 12.0 0.238636363636 0.166666666667 0.0950266487328\n",
      "inputs at 26 are: 20.0 1.05263157895 -12.0 0.212765957447 0.22619047619 -0.0594578884746\n",
      "inputs at 27 are: 10.0 0.625 -12.0 0.25 0.228571428571 -0.0353031212818\n",
      "inputs at 28 are: 17.0 0.944444444444 3.0 0.158878504673 0.25 0.0833368625792\n",
      "inputs at 29 are: 9.0 0.6 -13.0 0.209302325581 0.238095238095 -0.0360673760222\n",
      "inputs at 30 are: 20.0 3.33333333333 11.0 0.215053763441 0.153846153846 0.0980433677402\n",
      "inputs at 31 are: 15.0 0.882352941176 -2.0 0.258620689655 0.202380952381 -0.325825917414\n",
      "inputs at 32 are: 17.0 0.772727272727 -2.0 0.173469387755 0.224489795918 -0.285344515553\n",
      "inputs at 33 are: 16.0 1.45454545455 5.0 0.175824175824 0.144736842105 0.0855651209369\n",
      "inputs at 34 are: 34.0 1.25925925926 0.0 0.261538461538 0.188811188811 0.454181772132\n",
      "inputs at 35 are: 28.0 1.12 9.0 0.254545454545 0.20325203252 0.0395310858729\n",
      "inputs at 36 are: 21.0 1.3125 9.0 0.225806451613 0.228571428571 0.0463254912572\n",
      "inputs at 37 are: 22.0 1.57142857143 -9.0 0.289473684211 0.164705882353 -0.0944621752963\n",
      "inputs at 38 are: 39.0 1.95 0.0 0.307086614173 0.175438596491 0.703313832433\n",
      "inputs at 39 are: 18.0 1.63636363636 3.0 0.206896551724 0.171875 0.144391141581\n",
      "inputs at 40 are: 20.0 1.33333333333 3.0 0.194174757282 0.161290322581 0.117652041288\n",
      "inputs at 41 are: 19.0 1.05555555556 -8.0 0.223529411765 0.2 -0.0686291480393\n",
      "inputs at 42 are: 12.0 0.666666666667 9.0 0.173913043478 0.206896551724 0.0235304082577\n",
      "inputs at 43 are: 21.0 1.5 9.0 0.230769230769 0.21875 0.0529434185797\n",
      "inputs at 44 are: 19.0 1.26666666667 8.0 0.211111111111 0.223880597015 0.0496753063217\n",
      "inputs at 45 are: 9.0 0.529411764706 7.0 0.0957446808511 0.27868852459 0.0233573905499\n",
      "inputs at 46 are: 24.0 1.14285714286 3.0 0.224299065421 0.28 0.100844606819\n",
      "inputs at 47 are: 22.0 1.1 -6.0 0.236559139785 0.25 -0.0917471261133\n",
      "inputs at 48 are: 20.0 2.0 14.0 0.246913580247 0.25641025641 0.0470608165153\n",
      "inputs at 49 are: 14.0 0.608695652174 -2.0 0.14 0.270588235294 -0.224772661868\n",
      "inputs at 50 are: 11.0 0.55 -11.0 0.189655172414 0.266666666667 -0.0306960844603\n",
      "inputs at 51 are: 12.0 0.6 -15.0 0.1875 0.240963855422 -0.0618297874667\n",
      "inputs at 52 are: 33.0 1.32 0.0 0.248120300752 0.198412698413 0.476089363493\n",
      "inputs at 53 are: 20.0 1.0 -15.0 0.217391304348 0.19801980198 -0.103049645778\n",
      "inputs at 54 are: 12.0 0.705882352941 0.0 0.255319148936 0.22972972973 0.25459324251\n",
      "inputs at 55 are: 29.0 1.26086956522 0.0 0.223076923077 0.190082644628 0.454762567237\n",
      "inputs at 56 are: 15.0 0.833333333333 -3.0 0.141509433962 0.219512195122 -0.157884659037\n",
      "inputs at 57 are: 19.0 0.791666666667 -4.0 0.228915662651 0.272727272727 -0.102882689719\n",
      "inputs at 58 are: 17.0 4.25 14.0 0.2 0.117647058824 0.100004235095\n",
      "inputs at 59 are: 23.0 2.875 12.0 0.234693877551 0.170212765957 0.0780576043162\n",
      "inputs at 60 are: 22.0 3.66666666667 12.0 0.252873563218 0.153846153846 0.0995517272439\n",
      "inputs at 61 are: 22.0 0.916666666667 -5.0 0.252873563218 0.292682926829 -0.0922234510042\n",
      "inputs at 62 are: 23.0 1.21052631579 -2.0 0.219047619048 0.174311926606 -0.447010293715\n",
      "inputs at 63 are: 20.0 1.05263157895 -6.0 0.235294117647 0.240506329114 -0.0877962929314\n",
      "inputs at 64 are: 23.0 1.0 0.0 0.167883211679 0.188524590164 0.360673760222\n",
      "inputs at 65 are: 6.0 0.375 -14.0 0.142857142857 0.219178082192 -0.0262569007681\n",
      "inputs at 66 are: 19.0 0.95 4.0 0.19587628866 0.235294117647 0.0670616635343\n",
      "inputs at 67 are: 14.0 0.777777777778 4.0 0.12962962963 0.260869565217 0.0549042859345\n",
      "inputs at 68 are: 23.0 1.21052631579 3.0 0.23 0.19387755102 0.106815669064\n",
      "inputs at 69 are: 18.0 1.0 -6.0 0.209302325581 0.227848101266 -0.0834064782848\n",
      "inputs at 70 are: 11.0 1.57142857143 3.0 0.123595505618 0.127272727273 0.138661334375\n",
      "inputs at 71 are: 31.0 1.47619047619 3.0 0.264957264957 0.205882352941 0.130257617141\n",
      "inputs at 72 are: 22.0 2.75 4.0 0.241758241758 0.111111111111 0.194125868126\n",
      "inputs at 73 are: 25.0 1.25 -3.0 0.240384615385 0.20618556701 -0.236826988556\n",
      "inputs at 74 are: 22.0 1.1 -3.0 0.226804123711 0.19801980198 -0.208407749929\n",
      "inputs at 75 are: 23.0 1.91666666667 8.0 0.261363636364 0.230769230769 0.0751665819342\n",
      "inputs at 76 are: 12.0 1.2 12.0 0.130434782609 0.25 0.0325805652798\n",
      "inputs at 77 are: 23.0 1.21052631579 -4.0 0.247311827957 0.231707317073 -0.157316467382\n",
      "inputs at 78 are: 16.0 1.0 2.0 0.164948453608 0.16 0.117652041288\n",
      "inputs at 79 are: 24.0 2.18181818182 8.0 0.25 0.224489795918 0.0855651209369\n",
      "inputs at 80 are: 15.0 0.833333333333 6.0 0.15 0.209302325581 0.0420185861744\n",
      "inputs at 81 are: 15.0 0.833333333333 -10.0 0.25 0.253521126761 -0.0475831798491\n",
      "inputs at 82 are: 10.0 0.47619047619 -5.0 0.121951219512 0.216494845361 -0.0479082862359\n",
      "inputs at 83 are: 22.0 0.95652173913 0.0 0.171875 0.244680851064 0.344992292386\n",
      "inputs at 84 are: 17.0 0.809523809524 -3.0 0.184782608696 0.244186046512 -0.153373668779\n",
      "inputs at 85 are: 19.0 1.35714285714 5.0 0.19 0.166666666667 0.0798353137313\n",
      "inputs at 86 are: 9.0 2.25 2.0 0.109756097561 0.153846153846 0.264717092899\n",
      "inputs at 87 are: 19.0 0.904761904762 2.0 0.19 0.25 0.106447084975\n",
      "inputs at 88 are: 28.0 1.75 6.0 0.301075268817 0.213333333333 0.0882390309662\n",
      "inputs at 89 are: 14.0 1.16666666667 6.0 0.147368421053 0.146341463415 0.0588260206441\n",
      "inputs at 90 are: 7.0 0.368421052632 -13.0 0.152173913043 0.24358974359 -0.0221466343996\n",
      "inputs at 91 are: 11.0 0.647058823529 -13.0 0.21568627451 0.239436619718 -0.0388961898279\n",
      "inputs at 92 are: 18.0 2.57142857143 12.0 0.202247191011 0.170731707317 0.0698154970282\n",
      "inputs at 93 are: 14.0 0.933333333333 7.0 0.145833333333 0.230769230769 0.0411782144509\n",
      "inputs at 94 are: 26.0 1.625 -4.0 0.313253012048 0.190476190476 -0.211180257844\n",
      "inputs at 95 are: 11.0 1.0 9.0 0.118279569892 0.289473684211 0.0352956123865\n",
      "inputs at 96 are: 18.0 1.0 -4.0 0.225 0.211764705882 -0.12995708175\n",
      "inputs at 97 are: 17.0 1.0625 -8.0 0.25 0.216216216216 -0.0690806555922\n",
      "inputs at 98 are: 18.0 0.947368421053 2.0 0.209302325581 0.223529411765 0.111459828589\n",
      "inputs at 99 are: 12.0 0.8 9.0 0.127659574468 0.288461538462 0.0282364899092\n",
      "inputs at 100 are: 25.0 1.04166666667 -2.0 0.290697674419 0.292682926829 -0.384655596947\n",
      "inputs at 101 are: 15.0 1.25 10.0 0.170454545455 0.244897959184 0.0401086504392\n",
      "inputs at 102 are: 12.0 0.666666666667 3.0 0.110091743119 0.243243243243 0.0588260206441\n",
      "inputs at 103 are: 9.0 0.75 13.0 0.109756097561 0.260869565217 0.0189083637785\n",
      "inputs at 104 are: 23.0 1.53333333333 8.0 0.242105263158 0.208333333333 0.0601332655473\n",
      "inputs at 105 are: 10.0 0.555555555556 -12.0 0.169491525424 0.257142857143 -0.0313805522505\n",
      "inputs at 106 are: 9.0 0.45 0.0 0.136363636364 0.253164556962 0.1623031921\n",
      "inputs at 107 are: 32.0 1.28 -6.0 0.223776223776 0.227272727273 -0.106760292205\n",
      "inputs at 108 are: 11.0 0.55 -6.0 0.142857142857 0.25641025641 -0.0458735630567\n",
      "inputs at 109 are: 19.0 1.72727272727 14.0 0.22619047619 0.229166666667 0.040643432445\n",
      "inputs at 110 are: 15.0 1.875 14.0 0.185185185185 0.242424242424 0.0441195154831\n",
      "inputs at 111 are: 25.0 1.66666666667 -2.0 0.229357798165 0.214285714286 -0.615448955115\n",
      "inputs at 112 are: 22.0 1.22222222222 10.0 0.22 0.204545454545 0.0392173470961\n",
      "inputs at 113 are: 14.0 1.07692307692 10.0 0.166666666667 0.232142857143 0.0345551449938\n",
      "inputs at 114 are: 20.0 0.909090909091 7.0 0.21978021978 0.278481012658 0.0401086504392\n",
      "inputs at 115 are: 19.0 1.05555555556 -7.0 0.279411764706 0.219512195122 -0.0764036588533\n",
      "inputs at 116 are: 19.0 1.46153846154 -7.0 0.19587628866 0.228070175439 -0.105789681489\n",
      "inputs at 117 are: 32.0 1.18518518519 0.0 0.237037037037 0.259615384615 0.4274651973\n",
      "inputs at 118 are: 44.0 3.66666666667 5.0 0.444444444444 0.166666666667 0.215695409028\n",
      "inputs at 119 are: 39.0 1.18181818182 0.0 0.25 0.259842519685 0.426250807535\n",
      "inputs at 120 are: 31.0 1.19230769231 0.0 0.244094488189 0.25 0.430034098727\n",
      "inputs at 121 are: 24.0 1.41176470588 -4.0 0.24 0.269841269841 -0.183468821295\n",
      "inputs at 122 are: 13.0 0.590909090909 5.0 0.156626506024 0.275 0.0347608303806\n",
      "inputs at 123 are: 24.0 1.6 5.0 0.242424242424 0.220588235294 0.0941216330306\n",
      "inputs at 124 are: 20.0 1.0 -7.0 0.212765957447 0.232558139535 -0.0723824136505\n",
      "inputs at 125 are: 20.0 1.05263157895 -7.0 0.277777777778 0.223529411765 -0.076192014369\n",
      "inputs at 126 are: 22.0 1.46666666667 8.0 0.229166666667 0.185185185185 0.0575187757409\n",
      "inputs at 127 are: 20.0 1.53846153846 8.0 0.192307692308 0.213114754098 0.0603343801478\n",
      "inputs at 128 are: 22.0 1.04761904762 0.0 0.18487394958 0.196261682243 0.377848701185\n",
      "inputs at 129 are: 26.0 1.2380952381 2.0 0.252427184466 0.2625 0.145664432071\n",
      "inputs at 130 are: 30.0 1.5 -3.0 0.272727272727 0.2 -0.284192386267\n",
      "inputs at 131 are: 28.0 1.86666666667 6.0 0.28 0.234375 0.0941216330306\n",
      "inputs at 132 are: 19.0 1.05555555556 -8.0 0.287878787879 0.204545454545 -0.0686291480393\n",
      "inputs at 133 are: 10.0 0.666666666667 11.0 0.112359550562 0.272727272727 0.019608673548\n",
      "inputs at 134 are: 21.0 1.05 -2.0 0.205882352941 0.190476190476 -0.387732841722\n",
      "inputs at 135 are: 25.0 1.38888888889 -3.0 0.252525252525 0.214285714286 -0.263141098396\n",
      "inputs at 136 are: 21.0 1.3125 3.0 0.198113207547 0.164948453608 0.115813728143\n",
      "inputs at 137 are: 14.0 0.933333333333 9.0 0.170731707317 0.283018867925 0.0329425715607\n",
      "inputs at 138 are: 19.0 1.35714285714 7.0 0.19387755102 0.186666666667 0.0598764852985\n",
      "inputs at 139 are: 17.0 1.54545454545 14.0 0.2 0.234042553191 0.0363651763982\n",
      "inputs at 140 are: 24.0 1.5 5.0 0.242424242424 0.225352112676 0.0882390309662\n",
      "inputs at 141 are: 16.0 1.14285714286 10.0 0.190476190476 0.191780821918 0.0366707661158\n",
      "inputs at 142 are: 19.0 0.863636363636 -6.0 0.2375 0.258823529412 -0.0720328676096\n",
      "inputs at 143 are: 19.0 1.35714285714 8.0 0.20652173913 0.189189189189 0.0532235424875\n",
      "inputs at 144 are: 23.0 1.0 0.0 0.185483870968 0.234693877551 0.360673760222\n",
      "inputs at 145 are: 21.0 1.75 8.0 0.244186046512 0.153846153846 0.0686303574181\n",
      "inputs at 146 are: 21.0 1.90909090909 12.0 0.2625 0.25 0.0518327174906\n",
      "inputs at 147 are: 33.0 1.17857142857 0.0 0.268292682927 0.261682242991 0.425079788833\n",
      "inputs at 148 are: 8.0 0.470588235294 -11.0 0.16 0.242857142857 -0.0262640294848\n",
      "inputs at 149 are: 24.0 1.41176470588 -4.0 0.260869565217 0.2125 -0.183468821295\n",
      "inputs at 150 are: 23.0 1.21052631579 11.0 0.258426966292 0.180952380952 0.0356052230214\n",
      "inputs at 151 are: 18.0 1.2 11.0 0.21686746988 0.227272727273 0.0352956123865\n",
      "inputs at 152 are: 14.0 0.636363636364 -7.0 0.186666666667 0.222222222222 -0.0460615359594\n",
      "inputs at 153 are: 15.0 0.789473684211 -7.0 0.194805194805 0.211111111111 -0.0571440107767\n",
      "inputs at 154 are: 38.0 1.35714285714 0.0 0.255033557047 0.25 0.489485817444\n",
      "inputs at 155 are: 11.0 1.1 12.0 0.132530120482 0.185185185185 0.0298655181732\n",
      "inputs at 156 are: 26.0 1.73333333333 8.0 0.265306122449 0.254237288136 0.0679767349665\n",
      "inputs at 157 are: 7.0 0.4375 -14.0 0.12962962963 0.253968253968 -0.0306330508961\n",
      "inputs at 158 are: 23.0 1.76923076923 8.0 0.252747252747 0.1625 0.06938453717\n",
      "inputs at 159 are: 22.0 1.29411764706 -7.0 0.22 0.253731343284 -0.0936713588419\n",
      "inputs at 160 are: 18.0 1.0 11.0 0.236842105263 0.257142857143 0.0294130103221\n",
      "inputs at 161 are: 16.0 1.6 11.0 0.19512195122 0.227272727273 0.0470608165153\n",
      "inputs at 162 are: 21.0 1.23529411765 4.0 0.201923076923 0.246376811594 0.0872009247195\n",
      "inputs at 163 are: 23.0 1.35294117647 4.0 0.221153846154 0.242857142857 0.0955057746928\n",
      "inputs at 164 are: 28.0 1.55555555556 0.0 0.277227722772 0.193548387097 0.561048071457\n",
      "inputs at 165 are: 23.0 1.04545454545 2.0 0.211009174312 0.18487394958 0.122999861347\n",
      "inputs at 166 are: 11.0 0.458333333333 2.0 0.114583333333 0.237623762376 0.0539238522571\n",
      "inputs at 167 are: 23.0 1.04545454545 2.0 0.209090909091 0.229166666667 0.122999861347\n",
      "inputs at 168 are: 22.0 1.15789473684 2.0 0.22 0.211111111111 0.136228679386\n",
      "inputs at 169 are: 28.0 1.86666666667 6.0 0.282828282828 0.189873417722 0.0941216330306\n",
      "inputs at 170 are: 28.0 4.0 13.0 0.325581395349 0.184210526316 0.100844606819\n",
      "inputs at 171 are: 37.0 1.15625 -4.0 0.229813664596 0.188235294118 -0.150262875774\n",
      "inputs at 172 are: 17.0 0.85 -4.0 0.178947368421 0.20202020202 -0.110463519488\n",
      "inputs at 173 are: 28.0 1.4 -3.0 0.297872340426 0.194174757282 -0.265246227183\n",
      "inputs at 174 are: 23.0 1.15 6.0 0.244680851064 0.3125 0.0579856489206\n",
      "inputs at 175 are: 19.0 1.72727272727 5.0 0.220930232558 0.150684931507 0.101608581113\n",
      "inputs at 176 are: 28.0 1.86666666667 5.0 0.277227722772 0.174418604651 0.109808571869\n",
      "inputs at 177 are: 15.0 1.25 8.0 0.168539325843 0.22641509434 0.0490216838701\n",
      "inputs at 178 are: 13.0 1.08333333333 8.0 0.147727272727 0.266666666667 0.0424854593541\n",
      "inputs at 179 are: 15.0 0.789473684211 2.0 0.151515151515 0.231707317073 0.0928831904907\n",
      "inputs at 180 are: 25.0 1.25 2.0 0.255102040816 0.190476190476 0.14706505161\n",
      "inputs at 181 are: 29.0 1.11538461538 0.0 0.211678832117 0.245283018868 0.402289963325\n",
      "inputs at 182 are: 16.0 0.842105263158 7.0 0.163265306122 0.2375 0.0371532761963\n",
      "inputs at 183 are: 21.0 0.954545454545 8.0 0.272727272727 0.25 0.0374347404099\n",
      "inputs at 184 are: 16.0 1.23076923077 -12.0 0.181818181818 0.216666666667 -0.069519992678\n",
      "inputs at 185 are: 13.0 0.8125 -12.0 0.282608695652 0.225352112676 -0.0458940576663\n",
      "inputs at 186 are: 25.0 3.125 7.0 0.287356321839 0.129032258065 0.137873485885\n",
      "inputs at 187 are: 17.0 0.894736842105 7.0 0.17 0.2375 0.0394753559586\n",
      "inputs at 188 are: 26.0 1.18181818182 0.0 0.337662337662 0.285714285714 0.426250807535\n",
      "inputs at 189 are: 47.0 1.23684210526 -2.0 0.225961538462 0.218390804598 -0.456727908796\n",
      "inputs at 190 are: 18.0 0.75 -2.0 0.181818181818 0.279069767442 -0.276952029802\n",
      "inputs at 191 are: 21.0 1.4 8.0 0.233333333333 0.230769230769 0.0549042859345\n",
      "inputs at 192 are: 15.0 0.833333333333 8.0 0.151515151515 0.253521126761 0.0326811225801\n",
      "inputs at 193 are: 19.0 0.95 -3.0 0.20652173913 0.25 -0.179988511303\n",
      "inputs at 194 are: 21.0 1.10526315789 -3.0 0.23595505618 0.22619047619 -0.209404916197\n",
      "inputs at 195 are: 17.0 0.944444444444 -7.0 0.22972972973 0.246575342466 -0.0683611684477\n",
      "inputs at 196 are: 16.0 0.761904761905 -7.0 0.213333333333 0.2625 -0.0551485056385\n",
      "inputs at 197 are: 24.0 1.04347826087 -7.0 0.222222222222 0.267441860465 -0.0755294751136\n",
      "inputs at 198 are: 15.0 0.75 6.0 0.192307692308 0.253164556962 0.0378167275569\n",
      "inputs at 199 are: 24.0 1.26315789474 6.0 0.252631578947 0.283582089552 0.0636913306222\n",
      "inputs at 200 are: 24.0 1.5 8.0 0.272727272727 0.262295081967 0.0588260206441\n",
      "inputs at 201 are: 18.0 1.28571428571 8.0 0.1875 0.28 0.0504223034093\n",
      "inputs at 202 are: 22.0 2.0 12.0 0.275 0.268292682927 0.054300942133\n",
      "inputs at 203 are: 23.0 1.53333333333 6.0 0.219047619048 0.174418604651 0.0773141985609\n",
      "inputs at 204 are: 14.0 0.636363636364 -7.0 0.2 0.275 -0.0460615359594\n",
      "inputs at 205 are: 20.0 0.833333333333 -4.0 0.238095238095 0.235294117647 -0.108297568125\n",
      "inputs at 206 are: 16.0 1.0 7.0 0.175824175824 0.258064516129 0.0441195154831\n",
      "inputs at 207 are: 21.0 1.10526315789 4.0 0.214285714286 0.211111111111 0.0780218800122\n",
      "inputs at 208 are: 28.0 1.47368421053 14.0 0.245614035088 0.22619047619 0.0346763911165\n",
      "inputs at 209 are: 15.0 1.36363636364 -13.0 0.189873417722 0.282051282051 -0.0819713091414\n",
      "inputs at 210 are: 6.0 0.333333333333 -13.0 0.153846153846 0.264705882353 -0.0200374311235\n",
      "inputs at 211 are: 15.0 1.36363636364 7.0 0.182926829268 0.186440677966 0.0601629756588\n",
      "inputs at 212 are: 20.0 1.17647058824 7.0 0.208333333333 0.202380952381 0.0519053123331\n",
      "inputs at 213 are: 23.0 1.35294117647 7.0 0.242105263158 0.320754716981 0.059691109183\n",
      "inputs at 214 are: 20.0 1.0 -8.0 0.31746031746 0.253164556962 -0.0650170876162\n",
      "inputs at 215 are: 24.0 1.09090909091 3.0 0.228571428571 0.275 0.096260761054\n",
      "inputs at 216 are: 25.0 1.38888888889 3.0 0.233644859813 0.230769230769 0.122554209675\n",
      "inputs at 217 are: 18.0 0.72 -6.0 0.165137614679 0.235849056604 -0.0600526643651\n",
      "inputs at 218 are: 14.0 0.666666666667 4.0 0.197183098592 0.207920792079 0.0470608165153\n",
      "inputs at 219 are: 20.0 1.11111111111 4.0 0.2 0.195652173913 0.0784346941922\n",
      "inputs at 220 are: 15.0 0.652173913043 -6.0 0.172413793103 0.214953271028 -0.0543955293162\n",
      "inputs at 221 are: 11.0 0.5 -6.0 0.144736842105 0.226804123711 -0.0417032391424\n",
      "inputs at 222 are: 15.0 0.833333333333 -4.0 0.2 0.1875 -0.108297568125\n",
      "inputs at 223 are: 11.0 0.478260869565 -12.0 0.141025641026 0.2875 -0.0270145623722\n",
      "inputs at 224 are: 6.0 0.4 -12.0 0.133333333333 0.230769230769 -0.0225939976203\n",
      "inputs at 225 are: 19.0 3.16666666667 12.0 0.218390804598 0.166666666667 0.0859764917106\n",
      "inputs at 226 are: 23.0 0.958333333333 -4.0 0.267441860465 0.235294117647 -0.124542203344\n",
      "inputs at 227 are: 15.0 0.75 -14.0 0.178571428571 0.25 -0.0525138015362\n",
      "inputs at 228 are: 6.0 0.375 2.0 0.136363636364 0.238805970149 0.0441195154831\n",
      "inputs at 229 are: 22.0 1.46666666667 2.0 0.211538461538 0.185185185185 0.172556327223\n",
      "inputs at 230 are: 15.0 0.75 -8.0 0.217391304348 0.215053763441 -0.0487628157122\n",
      "inputs at 231 are: 20.0 1.17647058824 -8.0 0.27027027027 0.207317073171 -0.0764906913132\n",
      "inputs at 232 are: 16.0 1.06666666667 8.0 0.173913043478 0.208333333333 0.0418318369025\n",
      "inputs at 233 are: 26.0 0.962962962963 0.0 0.198473282443 0.25 0.347315472807\n",
      "inputs at 234 are: 30.0 1.5 2.0 0.288461538462 0.215053763441 0.176478061932\n",
      "inputs at 235 are: 17.0 1.30769230769 8.0 0.175257731959 0.26 0.0512842231256\n",
      "inputs at 236 are: 21.0 1.4 8.0 0.216494845361 0.277777777778 0.0549042859345\n",
      "inputs at 237 are: 15.0 0.833333333333 -6.0 0.185185185185 0.191489361702 -0.0695053985707\n",
      "inputs at 238 are: 22.0 0.95652173913 0.0 0.191304347826 0.237113402062 0.344992292386\n",
      "inputs at 239 are: 25.0 1.92307692308 10.0 0.277777777778 0.185714285714 0.0617056160603\n",
      "inputs at 240 are: 20.0 2.22222222222 13.0 0.229885057471 0.272727272727 0.0560247815658\n",
      "inputs at 241 are: 27.0 1.42105263158 4.0 0.245454545455 0.215909090909 0.10031384573\n",
      "inputs at 242 are: 24.0 1.6 8.0 0.25 0.283018867925 0.0627477553537\n",
      "inputs at 243 are: 27.0 1.5 3.0 0.27 0.191489361702 0.132358546449\n",
      "inputs at 244 are: 16.0 1.33333333333 11.0 0.179775280899 0.260869565217 0.0392173470961\n",
      "inputs at 245 are: 21.0 1.3125 5.0 0.203883495146 0.242424242424 0.0772091520954\n",
      "inputs at 246 are: 19.0 0.95 6.0 0.215909090909 0.25974025974 0.0479011882388\n",
      "inputs at 247 are: 22.0 1.69230769231 11.0 0.247191011236 0.309523809524 0.049775863622\n",
      "inputs at 248 are: 20.0 1.66666666667 4.0 0.21978021978 0.24 0.117652041288\n",
      "inputs at 249 are: 24.0 1.2 -10.0 0.230769230769 0.285714285714 -0.0685197789826\n",
      "inputs at 250 are: 7.0 0.368421052632 -10.0 0.120689655172 0.256756756757 -0.0210367742491\n",
      "inputs at 251 are: 24.0 1.6 7.0 0.263736263736 0.220588235294 0.070591224773\n",
      "inputs at 252 are: 25.0 2.08333333333 7.0 0.265957446809 0.151898734177 0.0919156572564\n",
      "inputs at 253 are: 31.0 1.06896551724 9.0 0.210884353741 0.27619047619 0.0377297925511\n",
      "inputs at 254 are: 13.0 1.0 9.0 0.14606741573 0.236363636364 0.0352956123865\n",
      "inputs at 255 are: 23.0 1.04545454545 2.0 0.225490196078 0.268292682927 0.122999861347\n",
      "inputs at 256 are: 9.0 0.428571428571 -9.0 0.130434782609 0.230769230769 -0.0257624114444\n",
      "inputs at 257 are: 19.0 2.11111111111 0.0 0.228915662651 0.225 0.761422382691\n",
      "inputs at 258 are: 23.0 0.884615384615 0.0 0.175572519084 0.288888888889 0.31905755712\n",
      "inputs at 259 are: 18.0 0.947368421053 -8.0 0.25 0.263888888889 -0.0615951356364\n",
      "inputs at 260 are: 18.0 1.0 5.0 0.174757281553 0.189473684211 0.0588260206441\n",
      "inputs at 261 are: 15.0 0.75 6.0 0.15306122449 0.215053763441 0.0378167275569\n",
      "inputs at 262 are: 24.0 1.71428571429 11.0 0.279069767442 0.264150943396 0.0504223034093\n",
      "inputs at 263 are: 18.0 0.782608695652 -2.0 0.178217821782 0.283950617284 -0.288993422402\n",
      "inputs at 264 are: 9.0 0.428571428571 -7.0 0.121621621622 0.225806451613 -0.0310210344217\n",
      "inputs at 265 are: 14.0 1.75 15.0 0.170731707317 0.285714285714 0.0386045760477\n",
      "inputs at 266 are: 11.0 0.785714285714 9.0 0.123595505618 0.184210526316 0.0277322668751\n",
      "inputs at 267 are: 14.0 0.823529411765 -9.0 0.202898550725 0.193181818182 -0.0495042415991\n",
      "inputs at 268 are: 17.0 0.85 -5.0 0.202380952381 0.246913580247 -0.0855162909311\n",
      "inputs at 269 are: 30.0 3.33333333333 9.0 0.3125 0.121621621622 0.117652041288\n",
      "inputs at 270 are: 25.0 3.57142857143 11.0 0.280898876404 0.159090909091 0.105046465436\n",
      "inputs at 271 are: 23.0 0.958333333333 0.0 0.186991869919 0.258064516129 0.34564568688\n",
      "inputs at 272 are: 23.0 1.35294117647 5.0 0.232323232323 0.195402298851 0.0795881455773\n",
      "inputs at 273 are: 19.0 1.1875 5.0 0.182692307692 0.186046511628 0.0698558995149\n",
      "inputs at 274 are: 20.0 1.11111111111 -7.0 0.30303030303 0.225 -0.0804249040562\n",
      "inputs at 275 are: 15.0 0.833333333333 4.0 0.159574468085 0.246575342466 0.0588260206441\n"
     ]
    }
   ],
   "source": [
    "tz , iz =  get_targs(TENZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 20.           1.42857143  -7.           0.21052632   0.18421053\n",
      "    -0.10340345]\n",
      "  [ 15.           0.78947368  12.           0.21126761   0.21590909\n",
      "     0.02143458]\n",
      "  [ 15.           1.66666667  12.           0.17647059   0.16981132\n",
      "     0.04525079]\n",
      "  ..., \n",
      "  [ 19.           1.1875       5.           0.18269231   0.18604651\n",
      "     0.0698559 ]\n",
      "  [ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]\n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]]\n",
      "\n",
      " [[ 15.           0.78947368  12.           0.21126761   0.21590909\n",
      "     0.02143458]\n",
      "  [ 15.           1.66666667  12.           0.17647059   0.16981132\n",
      "     0.04525079]\n",
      "  [ 14.           2.33333333   3.           0.1686747    0.20689655\n",
      "     0.20589107]\n",
      "  ..., \n",
      "  [ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]\n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]]\n",
      "\n",
      " [[ 15.           1.66666667  12.           0.17647059   0.16981132\n",
      "     0.04525079]\n",
      "  [ 14.           2.33333333   3.           0.1686747    0.20689655\n",
      "     0.20589107]\n",
      "  [ 17.           0.89473684   3.           0.1588785    0.22093023\n",
      "     0.07895071]\n",
      "  ..., \n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]]\n",
      "\n",
      " ..., \n",
      " [[ 19.           1.1875       5.           0.18269231   0.18604651\n",
      "     0.0698559 ]\n",
      "  [ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]\n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  ..., \n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]]\n",
      "\n",
      " [[ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]\n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]]]\n",
      "...../////////////////////////................\n",
      "[[[ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  [ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]\n",
      "  [ 19.           1.1875       5.           0.18269231   0.18604651\n",
      "     0.0698559 ]\n",
      "  ..., \n",
      "  [ 15.           1.66666667  12.           0.17647059   0.16981132\n",
      "     0.04525079]\n",
      "  [ 15.           0.78947368  12.           0.21126761   0.21590909\n",
      "     0.02143458]\n",
      "  [ 20.           1.42857143  -7.           0.21052632   0.18421053\n",
      "    -0.10340345]]\n",
      "\n",
      " [[  0.           0.           0.           0.           0.           0.        ]\n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  [ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]\n",
      "  ..., \n",
      "  [ 14.           2.33333333   3.           0.1686747    0.20689655\n",
      "     0.20589107]\n",
      "  [ 15.           1.66666667  12.           0.17647059   0.16981132\n",
      "     0.04525079]\n",
      "  [ 15.           0.78947368  12.           0.21126761   0.21590909\n",
      "     0.02143458]]\n",
      "\n",
      " [[  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  ..., \n",
      "  [ 17.           0.89473684   3.           0.1588785    0.22093023\n",
      "     0.07895071]\n",
      "  [ 14.           2.33333333   3.           0.1686747    0.20689655\n",
      "     0.20589107]\n",
      "  [ 15.           1.66666667  12.           0.17647059   0.16981132\n",
      "     0.04525079]]\n",
      "\n",
      " ..., \n",
      " [[  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  ..., \n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  [ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]\n",
      "  [ 19.           1.1875       5.           0.18269231   0.18604651\n",
      "     0.0698559 ]]\n",
      "\n",
      " [[  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [ 15.           0.83333333   4.           0.15957447   0.24657534\n",
      "     0.05882602]\n",
      "  [ 20.           1.11111111  -7.           0.3030303    0.225       -0.0804249 ]]\n",
      "\n",
      " [[  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(iz)\n",
    "print(\"...../////////////////////////................\")\n",
    "print( iz[:,::-1,:])\n",
    "\n",
    "# SO IT WORKS FOR REAL\n",
    "iz = iz[:,::-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to normalize the input data as they are being ran through a sigmoid / tanh function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def normalize(x_train):\n",
    "    '''\n",
    "    normalizes each column...\n",
    "    '''\n",
    "    \n",
    "    # need to normalize x_train:\n",
    "    for thr in range(np.shape(x_train)[2]):\n",
    "        for col in range(np.shape(x_train)[1]): # iterate over each column:\n",
    "            for row in range(len(x_train)):\n",
    "                if math.isnan( ( x_train[row,col,thr] - np.min( x_train[:,col,thr])) / (np.max( x_train[:,col,thr]) -  np.min(x_train[:,col])) ) == True:\n",
    "                    x_train[row,col,thr] = 0\n",
    "                else:\n",
    "                    x_train[row,col,thr] = ( x_train[row,col,thr] - np.min( x_train[:,col,thr])) / (np.max( x_train[:,col,thr]) -  np.min(x_train[:,col]))\n",
    "               \n",
    "                    \n",
    "    return(x_train)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Time!\n",
    "\n",
    "Now that we have our input data and targets we can use these to split into test and training data set, then run them through an LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_train(inputs, targs, perc = 30):\n",
    "    '''\n",
    "    Take input and target data then split them by the given percentage ratio\n",
    "    \"percent\" must be between 1 and 50 - signalling no more training data splitting than testing \n",
    "    '''\n",
    "    N = np.shape(inputs)[0]\n",
    "    N_train = int( ((100 - perc) / 100 ) * N )\n",
    "    # train set...\n",
    "    train_i = inputs[:N_train,:,:]\n",
    "    train_t = targs[:N_train]\n",
    "    # test set...\n",
    "    test_i =  inputs[N_train+1:,:,:]\n",
    "    test_t = targs[N_train+1:]\n",
    "    \n",
    "    return(train_i,train_t,test_i,test_t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_i,train_t,test_i,test_t = test_train(iz,tz, 30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data normalization:\n",
    "Please note that this is quite an expensive algorithm...\n",
    "'''\n",
    "trainI = normalize(train_i) \n",
    "testI = normalize(test_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## keras imports \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_neurons = np.shape(train_i)[2]   \n",
    "out_neurons = 1 # since regression output  \n",
    "hidden_neurons = 3 # since 6 variables encoded...\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(LSTM(output_dim=hidden_neurons, input_dim=in_neurons, return_sequences=False)) \n",
    "model.add(Dense(output_dim=out_neurons, input_dim=hidden_neurons))\n",
    "model.add(Activation(\"linear\"))  \n",
    "model.compile(loss=\"mean_squared_error\", optimizer= 'rmsprop' )  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a model trained on fnx's data - will predict his next form...\n",
    "#np.shape(np.array(train_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 183 samples, validate on 10 samples\n",
      "Epoch 1/3\n",
      "183/183 [==============================] - 108s - loss: 0.0272 - val_loss: 0.0499\n",
      "Epoch 2/3\n",
      "183/183 [==============================] - 107s - loss: 0.0256 - val_loss: 0.0520\n",
      "Epoch 3/3\n",
      "183/183 [==============================] - 111s - loss: 0.0236 - val_loss: 0.0535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff57f5b5c0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainI, np.array(train_t) , batch_size=1, nb_epoch = 3, validation_split=0.05)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Time to predict some stuff LOL... :P\n",
    "'''\n",
    "predicted = model.predict(testI)  \n",
    "rmse = np.sqrt(((predicted - test_t) ** 2).mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting predicted Form vs Real\n",
    "\n",
    "Blue is the real, green is predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ff589166a0>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRFLo/wHaIRC//7QLPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ff5b9de9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(pd.DataFrame(test_t[:len(predicted)-2]))\n",
    "ax.plot(pd.DataFrame(predicted[:len(predicted)-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SAVE THE MODEL:\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('player_performance_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "#del model  # deletes the existing model - uncomment when needed\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one - uncomment when needed\n",
    "model = load_model('player_performance_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load back in the model\n",
    "model = load_model('player_performance_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Demonstration\n",
    "\n",
    "Below shows some examples of uses from this API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.23727443252\n",
      "4.04407206672\n",
      "5.49569404148\n"
     ]
    }
   ],
   "source": [
    "# test form calculation with different scenarios...\n",
    "# this demonstrates the use of form_calc...\n",
    "'''\n",
    "good - easy win\n",
    "'''\n",
    "p1 = form_calc( K=22,D=4,TK = 23+22+13+12+13, TD = 12+10+7+11+4,TS=16,ES = 1 )\n",
    "'''\n",
    "average... (slight win) Coldzera (SK) vs fnatic...\n",
    "'''\n",
    "p2 = form_calc( K=16,D=13,TK = 23+19+20+16+15, TD = 17+13+18+11+13,TS=16,ES = 9 )\n",
    "'''\n",
    "good... (slight) win) fnx (SK) vs fnatic...\n",
    "'''\n",
    "p3 = form_calc( K=23,D=13,TK = 23+19+20+16+15, TD = 17+13+18+11+13,TS=16,ES = 9 )\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below it might be useful to use a lookup table of players...\n",
    "\n",
    "This can be done by the **show_players()** command...\n",
    "Which returns a frame of each player.\n",
    "\n",
    "I haven't actually made this command yet - but is on its way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see the fellas we can use...\n",
    "show_players()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team_score API call\n",
    "\n",
    "This is used as player name followed by their last nth match. i.e. team_score(\"Get_RiGhT\",0) shows the team information from get_right's last game... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team score is: 16\n",
      "enemy score is: 11\n",
      "team kills are: 104.0\n",
      "team deaths are: 86.0\n"
     ]
    }
   ],
   "source": [
    "TS, ES, tk, td = team_score('GeT_RiGhT', 3)\n",
    "\n",
    "# then we can print these variables out:\n",
    "\n",
    "print('team score is:', TS )\n",
    "print('enemy score is:', ES )\n",
    "print('team kills are:', tk )\n",
    "print('team deaths are:', td)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below needs to be turned into a function - currently just testing that the data goes into the 3 dimensional array correctly...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manually defining N T and K although they will be decided by the data we are using from \"make_inputs\"\n",
    "N = np.shape(frame)[1]\n",
    "T = N # full sized time window...\n",
    "K = 6\n",
    "#\n",
    "tens = np.zeros((N,T,K)) # iniitalizes tensor shape... (From a given 'N', and known 'T' and 'K')\n",
    "#tens[0] = np.random.randn(T,K) # this is the dataframe at first sample...\n",
    "\n",
    "'''\n",
    "basically need to generate a new dataframe for each 'N'...\n",
    "N is given by the number of time steps as well - as T\n",
    "...\n",
    "each dataframe is a TxK array of zeros (using np.zeros)\n",
    "\n",
    "'''\n",
    "\n",
    "# loop over each N:\n",
    "#i =  # for the 5th one...\n",
    "for i in range(N):\n",
    "#if True == True: # comment this out for testing...\n",
    "    # generate dataframe of zeros\n",
    "    df = np.zeros((T,K)) # this is of size T by K...\n",
    "    \n",
    "    # add for the first one...\n",
    "    df[0] = frame.ix[:,N-1] # since frame is a pandas dataframe... (from make_inputs command...)\n",
    "    \n",
    "    for j in range(1,N-i):\n",
    "        df[j] = frame.ix[:,N-i-j] # since frame is a pandas dataframe... (from make_inputs command...)\n",
    "        \n",
    "    tens[i] = df # adds to the tensor...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells visualise how the new tensor appears and how the the original data frame looks - there are still a few patches that need fixing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d4a1f4e10216>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tens' is not defined"
     ]
    }
   ],
   "source": [
    "print(tens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0          1          2          3          4          5    \\\n",
      "0  21.000000  15.000000  20.000000  19.000000  23.000000  23.000000   \n",
      "1   1.166667   0.833333   1.111111   1.187500   1.352941   0.958333   \n",
      "2  -4.000000   4.000000  -7.000000   5.000000   5.000000   0.000000   \n",
      "3   0.238636   0.159574   0.303030   0.182692   0.232323   0.186992   \n",
      "4   0.193548   0.246575   0.225000   0.186047   0.195402   0.258065   \n",
      "5  -0.151617   0.058826  -0.080425   0.069856   0.079588   0.345646   \n",
      "\n",
      "         6          7          8          9      ...            267       268  \\\n",
      "0  25.000000  30.000000  17.000000  14.000000    ...      10.000000  6.000000   \n",
      "1   3.571429   3.333333   0.850000   0.823529    ...       0.526316  0.272727   \n",
      "2  11.000000   9.000000  -5.000000  -9.000000    ...     -11.000000 -9.000000   \n",
      "3   0.280899   0.312500   0.202381   0.202899    ...       0.172414  0.093750   \n",
      "4   0.159091   0.121622   0.246914   0.193182    ...       0.256757  0.229167   \n",
      "5   0.105046   0.117652  -0.085516  -0.049504    ...      -0.029374 -0.016394   \n",
      "\n",
      "         269        270        271        272        273        274  \\\n",
      "0  19.000000  17.000000  15.000000  17.000000  14.000000  15.000000   \n",
      "1   0.904762   1.062500   0.750000   0.894737   2.333333   1.666667   \n",
      "2  -6.000000  -6.000000 -12.000000   3.000000   3.000000  12.000000   \n",
      "3   0.253333   0.346939   0.192308   0.158879   0.168675   0.176471   \n",
      "4   0.265823   0.216216   0.266667   0.220930   0.206897   0.169811   \n",
      "5  -0.075463  -0.088619  -0.042364   0.078951   0.205891   0.045251   \n",
      "\n",
      "         275        276  \n",
      "0  15.000000  20.000000  \n",
      "1   0.789474   1.428571  \n",
      "2  12.000000  -7.000000  \n",
      "3   0.211268   0.210526  \n",
      "4   0.215909   0.184211  \n",
      "5   0.021435  -0.103403  \n",
      "\n",
      "[6 rows x 277 columns]\n"
     ]
    }
   ],
   "source": [
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "[[[1, 2], [1, 2]]]\n"
     ]
    }
   ],
   "source": [
    "#n = 1 # number of samples...\n",
    "#df = np.array(df)\n",
    "#KK = np.reshape(df,(n,np.shape(df)[1],np.shape(df)[0]) )\n",
    "#np.shape(KK)\n",
    "J = [[[[1,2],[1,2]]],[[[1,2]]]]\n",
    "J[0]\n",
    "print(np.shape(J))\n",
    "#get_tensor(df,2)\n",
    "print(J[0])\n",
    "#print(KK[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537\n"
     ]
    }
   ],
   "source": [
    "#np.shape((player_data('GeT_RiGhT').reset_index(drop=True) ))[0]\n",
    "df = make_inputs('GeT_RiGhT')\n",
    "df.to_csv('getright data.csv') # save this to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# win ratio metric:\n",
    "\n",
    "def win_ratio(team):\n",
    "    # get team data...\n",
    "    dato = team_data(team).reset_index(drop=True)\n",
    "    #print('The subset data length is:', len(dato))\n",
    "    # then iterate over every row:\n",
    "    WL = []\n",
    "    #print(np.shape(data)[0])\n",
    "    for row in range(len(dato)):\n",
    "        # print(dat.ix[row,5])    # this here jsut for testing...\n",
    "        if (dato.ix[row,5]) == team : # this is if they are the winning team - name is in the winning team col...\n",
    "            WL.append(1)\n",
    "            \n",
    "        else:\n",
    "            WL.append(0)\n",
    "            \n",
    "    #return(WL)\n",
    "    return(np.sum(WL)/len(WL), np.std(WL)) # this is the Win ratio metric... and variance (STD)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.60546875, 0.48874977521574114)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_ratio('EnVyUs') # returns mean and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile a lookup table of each team - unique team ID and team name...\n",
    "\n",
    "def table():\n",
    "    '''\n",
    "    This defines the total unique teams and their idx numbers...\n",
    "    '''\n",
    "    # total teams\n",
    "    teams = pd.concat([dat.ix[:,3],dat.ix[:,4]])\n",
    "    teams= teams.reset_index(drop=True)\n",
    "    teams = pd.DataFrame(pd.unique(teams))\n",
    "    print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0\n",
      "0                sylloM\n",
      "1          Epsilon Hype\n",
      "2             Fragsport\n",
      "3       ALTERNATE aTTaX\n",
      "4                  LDLC\n",
      "5              NiceShot\n",
      "6                  ENCE\n",
      "7               Epsilon\n",
      "8                fnatic\n",
      "9                    SK\n",
      "10          Preparation\n",
      "11                   G2\n",
      "12         dream[S]cape\n",
      "13        Natus Vincere\n",
      "14              B.O.O.T\n",
      "15             dignitas\n",
      "16          AVANT GARDE\n",
      "17                  NiP\n",
      "18            Athletico\n",
      "19                 New4\n",
      "20          HellRaisers\n",
      "21          VG.CyberZen\n",
      "22              onestop\n",
      "23               Exile5\n",
      "24           Virtus.pro\n",
      "25                TyLoo\n",
      "26            iGame.com\n",
      "27             FlipSid3\n",
      "28                 k1ck\n",
      "29            Immortals\n",
      "...                 ...\n",
      "1331     kRoGhs KanonEr\n",
      "1332           MIXERINO\n",
      "1333            madjicK\n",
      "1334   Eternal Conflict\n",
      "1335       The Flying V\n",
      "1336          Deponeret\n",
      "1337            Vikings\n",
      "1338            DEMGUNZ\n",
      "1339   kassad's Legends\n",
      "1340               WiLD\n",
      "1341           ZajebaNi\n",
      "1342       Clutchit.org\n",
      "1343    Hitbox.tv Staff\n",
      "1344  Russian eSF Staff\n",
      "1345       ProGaming.TD\n",
      "1346            Katrina\n",
      "1347            UNNAMED\n",
      "1348              Faith\n",
      "1349       fAlsu's team\n",
      "1350          GL SHAKER\n",
      "1351           GGWP.pro\n",
      "1352     Cyborg Factory\n",
      "1353           Outbreak\n",
      "1354              ZYLEX\n",
      "1355              fusd0\n",
      "1356            EZpeezy\n",
      "1357             Armata\n",
      "1358              Cytik\n",
      "1359          Imaginary\n",
      "1360      Faroe Islands\n",
      "\n",
      "[1361 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
